nohup: ignoring input
Using TensorFlow backend.
/home/lechongzhou/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)
read data...
x_train shape: (39711, 64, 64, 3)
y_train shape: (39711, 147)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 64, 64, 3)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 64, 64, 16)   448         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 64, 64, 16)   64          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 64, 64, 16)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 64, 64, 16)   2320        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 64, 64, 16)   64          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 64, 64, 16)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 64, 64, 16)   2320        activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 64, 64, 16)   64          conv2d_3[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 64, 64, 16)   0           activation_1[0][0]               
                                                                 batch_normalization_3[0][0]      
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 64, 64, 16)   0           add_1[0][0]                      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 64, 64, 16)   2320        activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 64, 64, 16)   64          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 64, 64, 16)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 64, 64, 16)   2320        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 64, 64, 16)   64          conv2d_5[0][0]                   
__________________________________________________________________________________________________
add_2 (Add)                     (None, 64, 64, 16)   0           activation_3[0][0]               
                                                                 batch_normalization_5[0][0]      
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 64, 64, 16)   0           add_2[0][0]                      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 64, 64, 16)   2320        activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 64, 64, 16)   64          conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 64, 64, 16)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 64, 64, 16)   2320        activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 64, 64, 16)   64          conv2d_7[0][0]                   
__________________________________________________________________________________________________
add_3 (Add)                     (None, 64, 64, 16)   0           activation_5[0][0]               
                                                                 batch_normalization_7[0][0]      
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 64, 64, 16)   0           add_3[0][0]                      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 32)   4640        activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 32, 32, 32)   128         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 32, 32, 32)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 32, 32, 32)   9248        activation_8[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 32, 32, 32)   544         activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 32, 32, 32)   128         conv2d_9[0][0]                   
__________________________________________________________________________________________________
add_4 (Add)                     (None, 32, 32, 32)   0           conv2d_10[0][0]                  
                                                                 batch_normalization_9[0][0]      
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 32, 32, 32)   0           add_4[0][0]                      
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 32, 32, 32)   9248        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 32, 32, 32)   128         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 32, 32, 32)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 32, 32, 32)   9248        activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 32, 32, 32)   128         conv2d_12[0][0]                  
__________________________________________________________________________________________________
add_5 (Add)                     (None, 32, 32, 32)   0           activation_9[0][0]               
                                                                 batch_normalization_11[0][0]     
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 32, 32, 32)   0           add_5[0][0]                      
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 32, 32, 32)   9248        activation_11[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 32, 32, 32)   128         conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 32, 32, 32)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 32, 32, 32)   9248        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 32, 32, 32)   128         conv2d_14[0][0]                  
__________________________________________________________________________________________________
add_6 (Add)                     (None, 32, 32, 32)   0           activation_11[0][0]              
                                                                 batch_normalization_13[0][0]     
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 32, 32, 32)   0           add_6[0][0]                      
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 16, 16, 64)   18496       activation_13[0][0]              
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 16, 16, 64)   256         conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 16, 16, 64)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 16, 16, 64)   36928       activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 16, 16, 64)   2112        activation_13[0][0]              
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 16, 16, 64)   256         conv2d_16[0][0]                  
__________________________________________________________________________________________________
add_7 (Add)                     (None, 16, 16, 64)   0           conv2d_17[0][0]                  
                                                                 batch_normalization_15[0][0]     
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 16, 16, 64)   0           add_7[0][0]                      
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 16, 16, 64)   36928       activation_15[0][0]              
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 16, 16, 64)   256         conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 16, 16, 64)   0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 16, 16, 64)   36928       activation_16[0][0]              
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 16, 16, 64)   256         conv2d_19[0][0]                  
__________________________________________________________________________________________________
add_8 (Add)                     (None, 16, 16, 64)   0           activation_15[0][0]              
                                                                 batch_normalization_17[0][0]     
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 16, 16, 64)   0           add_8[0][0]                      
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 16, 16, 64)   36928       activation_17[0][0]              
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 16, 16, 64)   256         conv2d_20[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 16, 16, 64)   0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 16, 16, 64)   36928       activation_18[0][0]              
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 16, 16, 64)   256         conv2d_21[0][0]                  
__________________________________________________________________________________________________
add_9 (Add)                     (None, 16, 16, 64)   0           activation_17[0][0]              
                                                                 batch_normalization_19[0][0]     
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 16, 16, 64)   0           add_9[0][0]                      
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 4, 4, 64)     0           activation_19[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 4, 4, 64)     0           average_pooling2d_1[0][0]        
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 1024)         0           dropout_1[0][0]                  
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 147)          150675      flatten_1[0][0]                  
==================================================================================================
Total params: 424,467
Trainable params: 423,091
Non-trainable params: 1,376
__________________________________________________________________________________________________
Using real-time data augmentation.
Epoch 1/500
2018-09-19 21:40:44.627754: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-09-19 21:40:44.743575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-19 21:40:44.744160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:04.0
totalMemory: 11.17GiB freeMemory: 11.09GiB
2018-09-19 21:40:44.744251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
Epoch 00001: val_acc improved from -inf to 0.10900, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_2_model.001.0.108996.h5
 - 176s - loss: 4.6121 - acc: 0.0746 - val_loss: 4.3492 - val_acc: 0.1090
Epoch 2/500
Epoch 00002: val_acc improved from 0.10900 to 0.18400, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_2_model.002.0.184002.h5
 - 170s - loss: 3.7936 - acc: 0.1776 - val_loss: 3.8138 - val_acc: 0.1840
Epoch 3/500
Epoch 00003: val_acc improved from 0.18400 to 0.24065, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_2_model.003.0.240653.h5
 - 171s - loss: 3.3564 - acc: 0.2469 - val_loss: 3.4187 - val_acc: 0.2407
Epoch 4/500
Epoch 00004: val_acc improved from 0.24065 to 0.26286, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_2_model.004.0.262860.h5
 - 170s - loss: 3.1043 - acc: 0.2947 - val_loss: 3.3914 - val_acc: 0.2629
Epoch 5/500
Epoch 00005: val_acc improved from 0.26286 to 0.32676, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_2_model.005.0.326762.h5
 - 171s - loss: 2.9203 - acc: 0.3313 - val_loss: 2.9833 - val_acc: 0.3268
Epoch 6/500
Epoch 00006: val_acc did not improve
 - 170s - loss: 2.7798 - acc: 0.3556 - val_loss: 3.0486 - val_acc: 0.3225
Epoch 7/500
Epoch 00007: val_acc improved from 0.32676 to 0.33039, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_2_model.007.0.330387.h5
 - 170s - loss: 2.6593 - acc: 0.3850 - val_loss: 3.0204 - val_acc: 0.3304
Epoch 8/500
Epoch 00008: val_acc improved from 0.33039 to 0.35939, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_2_model.008.0.359393.h5
 - 171s - loss: 2.5511 - acc: 0.4032 - val_loss: 2.8514 - val_acc: 0.3594
Epoch 9/500
Epoch 00009: val_acc improved from 0.35939 to 0.36891, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_2_model.009.0.368910.h5
 - 171s - loss: 2.4577 - acc: 0.4266 - val_loss: 2.8099 - val_acc: 0.3689
Epoch 10/500
Epoch 00010: val_acc improved from 0.36891 to 0.39792, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_2_model.010.0.397915.h5
 - 171s - loss: 2.3876 - acc: 0.4422 - val_loss: 2.6327 - val_acc: 0.3979
Epoch 11/500
Epoch 00011: val_acc improved from 0.39792 to 0.40335, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_2_model.011.0.403354.h5
 - 171s - loss: 2.3086 - acc: 0.4584 - val_loss: 2.6532 - val_acc: 0.4034
Epoch 12/500
Epoch 00012: val_acc did not improve
 - 171s - loss: 2.2501 - acc: 0.4719 - val_loss: 2.8550 - val_acc: 0.3648
Epoch 13/500
Epoch 00013: val_acc did not improve
 - 171s - loss: 2.1901 - acc: 0.4866 - val_loss: 3.2528 - val_acc: 0.3245
Epoch 14/500
Epoch 00014: val_acc improved from 0.40335 to 0.41468, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_2_model.014.0.414684.h5
 - 171s - loss: 2.1372 - acc: 0.5013 - val_loss: 2.6446 - val_acc: 0.4147
Epoch 15/500
Epoch 00015: val_acc did not improve
 - 171s - loss: 2.0898 - acc: 0.5108 - val_loss: 3.0214 - val_acc: 0.3730
Epoch 16/500
Epoch 00016: val_acc did not improve
 - 171s - loss: 2.0460 - acc: 0.5197 - val_loss: 2.8071 - val_acc: 0.3886
Epoch 17/500
Epoch 00017: val_acc improved from 0.41468 to 0.44233, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_2_model.017.0.442329.h5
 - 171s - loss: 1.8478 - acc: 0.5714 - val_loss: 2.5426 - val_acc: 0.4423
Epoch 18/500
Epoch 00018: val_acc improved from 0.44233 to 0.46091, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_2_model.018.0.460911.h5
 - 171s - loss: 1.8013 - acc: 0.5770 - val_loss: 2.4411 - val_acc: 0.4609
Epoch 19/500
Epoch 00019: val_acc did not improve
 - 171s - loss: 1.7634 - acc: 0.5839 - val_loss: 2.6270 - val_acc: 0.4296
Epoch 20/500
Epoch 00020: val_acc improved from 0.46091 to 0.46340, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_2_model.020.0.463404.h5
 - 171s - loss: 1.7356 - acc: 0.5914 - val_loss: 2.4678 - val_acc: 0.4634
Epoch 21/500
Epoch 00021: val_acc improved from 0.46340 to 0.47564, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_2_model.021.0.475640.h5
 - 171s - loss: 1.7067 - acc: 0.5961 - val_loss: 2.3734 - val_acc: 0.4756
Epoch 22/500
Epoch 00022: val_acc did not improve
 - 171s - loss: 1.6776 - acc: 0.6027 - val_loss: 2.4655 - val_acc: 0.4673
Epoch 23/500
Epoch 00023: val_acc did not improve
 - 171s - loss: 1.6616 - acc: 0.6059 - val_loss: 2.4525 - val_acc: 0.4591
Epoch 24/500
Epoch 00024: val_acc did not improve
 - 171s - loss: 1.6337 - acc: 0.6148 - val_loss: 2.4304 - val_acc: 0.4661
Epoch 25/500
Epoch 00025: val_acc did not improve
 - 171s - loss: 1.6100 - acc: 0.6187 - val_loss: 2.4758 - val_acc: 0.4709
Epoch 26/500
Epoch 00026: val_acc did not improve
 - 170s - loss: 1.5939 - acc: 0.6220 - val_loss: 2.4322 - val_acc: 0.4743
Epoch 27/500
Epoch 00027: val_acc did not improve
 - 171s - loss: 1.5674 - acc: 0.6291 - val_loss: 2.4792 - val_acc: 0.4684
Epoch 28/500
Epoch 00028: val_acc improved from 0.47564 to 0.49966, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_2_model.028.0.499660.h5
 - 171s - loss: 1.4364 - acc: 0.6606 - val_loss: 2.3165 - val_acc: 0.4997
Epoch 29/500
Epoch 00029: val_acc did not improve
 - 171s - loss: 1.4164 - acc: 0.6655 - val_loss: 2.3952 - val_acc: 0.4861
Epoch 30/500
Epoch 00030: val_acc did not improve
 - 171s - loss: 1.3987 - acc: 0.6673 - val_loss: 2.4072 - val_acc: 0.4915
Epoch 31/500
Epoch 00031: val_acc did not improve
 - 170s - loss: 1.3874 - acc: 0.6711 - val_loss: 2.4491 - val_acc: 0.4788
Epoch 32/500
Epoch 00032: val_acc did not improve
 - 171s - loss: 1.3713 - acc: 0.6762 - val_loss: 2.3892 - val_acc: 0.4972
Epoch 33/500
Epoch 00033: val_acc did not improve
 - 171s - loss: 1.3574 - acc: 0.6806 - val_loss: 2.4364 - val_acc: 0.4838
Epoch 34/500
Epoch 00034: val_acc did not improve
 - 171s - loss: 1.3474 - acc: 0.6794 - val_loss: 2.4664 - val_acc: 0.4811
Epoch 35/500
Epoch 00035: val_acc improved from 0.49966 to 0.50011, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_2_model.035.0.500113.h5
 - 170s - loss: 1.2798 - acc: 0.6967 - val_loss: 2.3689 - val_acc: 0.5001
Epoch 36/500
Epoch 00036: val_acc did not improve
 - 169s - loss: 1.2480 - acc: 0.7048 - val_loss: 2.3780 - val_acc: 0.4981
Epoch 37/500
Epoch 00037: val_acc did not improve
 - 170s - loss: 1.2465 - acc: 0.7046 - val_loss: 2.4047 - val_acc: 0.4917
Epoch 38/500
Epoch 00038: val_acc improved from 0.50011 to 0.50510, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_2_model.038.0.505099.h5
 - 171s - loss: 1.2455 - acc: 0.7045 - val_loss: 2.3859 - val_acc: 0.5051
Epoch 39/500
Epoch 00039: val_acc did not improve
 - 170s - loss: 1.2262 - acc: 0.7086 - val_loss: 2.4188 - val_acc: 0.4908
Epoch 40/500
Epoch 00040: val_acc improved from 0.50510 to 0.50691, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_2_model.040.0.506911.h5
 - 171s - loss: 1.1928 - acc: 0.7195 - val_loss: 2.3591 - val_acc: 0.5069
Epoch 41/500
Epoch 00041: val_acc did not improve
 - 171s - loss: 1.1842 - acc: 0.7196 - val_loss: 2.3583 - val_acc: 0.5033
Epoch 42/500
Epoch 00042: val_acc did not improve
 - 170s - loss: 1.1782 - acc: 0.7217 - val_loss: 2.3721 - val_acc: 0.4985
Epoch 43/500
Epoch 00043: val_acc did not improve
 - 171s - loss: 1.1801 - acc: 0.7218 - val_loss: 2.3650 - val_acc: 0.5017
Epoch 44/500
Epoch 00044: val_acc did not improve
 - 171s - loss: 1.1725 - acc: 0.7254 - val_loss: 2.3697 - val_acc: 0.5012
Epoch 45/500
Epoch 00045: val_acc did not improve
 - 171s - loss: 1.1468 - acc: 0.7317 - val_loss: 2.3651 - val_acc: 0.5017
Epoch 46/500
Epoch 00046: val_acc did not improve
 - 170s - loss: 1.1486 - acc: 0.7280 - val_loss: 2.3671 - val_acc: 0.5067
Epoch 47/500
Epoch 00047: val_acc did not improve
 - 171s - loss: 1.1457 - acc: 0.7300 - val_loss: 2.3638 - val_acc: 0.5053
Epoch 48/500
Epoch 00048: val_acc did not improve
 - 170s - loss: 1.1382 - acc: 0.7334 - val_loss: 2.3663 - val_acc: 0.5049
Epoch 49/500
Epoch 00049: val_acc did not improve
 - 170s - loss: 1.1370 - acc: 0.7339 - val_loss: 2.3680 - val_acc: 0.5040
Epoch 50/500
Epoch 00050: val_acc improved from 0.50691 to 0.50736, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_2_model.050.0.507365.h5
 - 171s - loss: 1.1273 - acc: 0.7349 - val_loss: 2.3649 - val_acc: 0.5074
Epoch 51/500
Epoch 00051: val_acc did not improve
 - 170s - loss: 1.1253 - acc: 0.7351 - val_loss: 2.3676 - val_acc: 0.5031
Epoch 52/500
Epoch 00052: val_acc did not improve
 - 171s - loss: 1.1204 - acc: 0.7363 - val_loss: 2.3688 - val_acc: 0.5049
Epoch 53/500
Epoch 00053: val_acc did not improve
 - 171s - loss: 1.1268 - acc: 0.7347 - val_loss: 2.3728 - val_acc: 0.5046
Epoch 54/500
Epoch 00054: val_acc did not improve
 - 171s - loss: 1.1207 - acc: 0.7349 - val_loss: 2.3700 - val_acc: 0.5031
Epoch 55/500
Epoch 00055: val_acc did not improve
 - 169s - loss: 1.1254 - acc: 0.7357 - val_loss: 2.3699 - val_acc: 0.5026
Epoch 56/500
Epoch 00056: val_acc did not improve
 - 169s - loss: 1.1100 - acc: 0.7385 - val_loss: 2.3684 - val_acc: 0.5044
Epoch 57/500
Epoch 00057: val_acc did not improve
 - 169s - loss: 1.1176 - acc: 0.7366 - val_loss: 2.3714 - val_acc: 0.5022
Epoch 58/500
Epoch 00058: val_acc did not improve
 - 169s - loss: 1.1146 - acc: 0.7401 - val_loss: 2.3726 - val_acc: 0.5037
Epoch 59/500
Epoch 00059: val_acc did not improve
 - 169s - loss: 1.1133 - acc: 0.7392 - val_loss: 2.3707 - val_acc: 0.5017
Epoch 60/500
Epoch 00060: val_acc did not improve
 - 170s - loss: 1.1114 - acc: 0.7382 - val_loss: 2.3701 - val_acc: 0.5035
Epoch 61/500
Epoch 00061: val_acc did not improve
 - 170s - loss: 1.1093 - acc: 0.7395 - val_loss: 2.3719 - val_acc: 0.5033
Epoch 62/500
Epoch 00062: val_acc did not improve
 - 170s - loss: 1.1100 - acc: 0.7372 - val_loss: 2.3735 - val_acc: 0.5028
Epoch 63/500
Epoch 00063: val_acc did not improve
 - 171s - loss: 1.1116 - acc: 0.7376 - val_loss: 2.3714 - val_acc: 0.5024
Epoch 64/500
Epoch 00064: val_acc did not improve
 - 171s - loss: 1.1080 - acc: 0.7376 - val_loss: 2.3723 - val_acc: 0.5024
Epoch 65/500
Epoch 00065: val_acc did not improve
 - 171s - loss: 1.1126 - acc: 0.7388 - val_loss: 2.3745 - val_acc: 0.5037
Epoch 66/500
Epoch 00066: val_acc did not improve
 - 170s - loss: 1.1010 - acc: 0.7396 - val_loss: 2.3746 - val_acc: 0.5033
Epoch 67/500
Epoch 00067: val_acc did not improve
 - 171s - loss: 1.1001 - acc: 0.7415 - val_loss: 2.3757 - val_acc: 0.5046
Epoch 68/500
Epoch 00068: val_acc did not improve
 - 170s - loss: 1.1043 - acc: 0.7414 - val_loss: 2.3747 - val_acc: 0.5035
Epoch 69/500
Epoch 00069: val_acc did not improve
 - 171s - loss: 1.1095 - acc: 0.7405 - val_loss: 2.3745 - val_acc: 0.5044
Epoch 70/500
Epoch 00070: val_acc did not improve
 - 171s - loss: 1.1073 - acc: 0.7388 - val_loss: 2.3733 - val_acc: 0.5040
Epoch 71/500
Epoch 00071: val_acc did not improve
 - 171s - loss: 1.1075 - acc: 0.7392 - val_loss: 2.3744 - val_acc: 0.5035
Epoch 72/500
Epoch 00072: val_acc did not improve
 - 170s - loss: 1.1053 - acc: 0.7402 - val_loss: 2.3737 - val_acc: 0.5046
Epoch 73/500
Epoch 00073: val_acc did not improve
 - 169s - loss: 1.1097 - acc: 0.7373 - val_loss: 2.3752 - val_acc: 0.5037
Epoch 74/500
Epoch 00074: val_acc did not improve
 - 170s - loss: 1.1103 - acc: 0.7384 - val_loss: 2.3748 - val_acc: 0.5042
Epoch 75/500
Epoch 00075: val_acc did not improve
 - 170s - loss: 1.1051 - acc: 0.7393 - val_loss: 2.3759 - val_acc: 0.5022
Epoch 76/500
Epoch 00076: val_acc did not improve
 - 170s - loss: 1.1037 - acc: 0.7383 - val_loss: 2.3755 - val_acc: 0.5042
Epoch 77/500
Epoch 00077: val_acc did not improve
 - 170s - loss: 1.1021 - acc: 0.7397 - val_loss: 2.3754 - val_acc: 0.5058
Epoch 78/500
Epoch 00078: val_acc did not improve
 - 170s - loss: 1.1064 - acc: 0.7421 - val_loss: 2.3751 - val_acc: 0.5035
Epoch 79/500
Epoch 00079: val_acc did not improve
 - 170s - loss: 1.0980 - acc: 0.7435 - val_loss: 2.3755 - val_acc: 0.5037
Epoch 80/500
Epoch 00080: val_acc did not improve
 - 170s - loss: 1.1040 - acc: 0.7413 - val_loss: 2.3747 - val_acc: 0.5044
Epoch 81/500
Epoch 00081: val_acc did not improve
 - 171s - loss: 1.1089 - acc: 0.7397 - val_loss: 2.3739 - val_acc: 0.5033
Epoch 82/500
Epoch 00082: val_acc did not improve
 - 170s - loss: 1.1031 - acc: 0.7414 - val_loss: 2.3751 - val_acc: 0.5056
Epoch 83/500
Epoch 00083: val_acc did not improve
 - 171s - loss: 1.1067 - acc: 0.7386 - val_loss: 2.3735 - val_acc: 0.5022
Epoch 84/500
Epoch 00084: val_acc did not improve
 - 171s - loss: 1.1027 - acc: 0.7407 - val_loss: 2.3745 - val_acc: 0.5044
Epoch 85/500
Epoch 00085: val_acc did not improve
 - 171s - loss: 1.1064 - acc: 0.7407 - val_loss: 2.3764 - val_acc: 0.5037
Epoch 86/500
Epoch 00086: val_acc did not improve
 - 170s - loss: 1.1089 - acc: 0.7401 - val_loss: 2.3764 - val_acc: 0.5037
Epoch 87/500
Epoch 00087: val_acc did not improve
 - 171s - loss: 1.1097 - acc: 0.7387 - val_loss: 2.3758 - val_acc: 0.5031
Epoch 88/500
Epoch 00088: val_acc did not improve
 - 171s - loss: 1.0973 - acc: 0.7427 - val_loss: 2.3750 - val_acc: 0.5035
Epoch 89/500
Epoch 00089: val_acc did not improve
 - 171s - loss: 1.1045 - acc: 0.7398 - val_loss: 2.3747 - val_acc: 0.5035
Epoch 90/500
