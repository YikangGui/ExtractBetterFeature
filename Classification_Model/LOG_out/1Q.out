nohup: ignoring input
Using TensorFlow backend.
/home/lechongzhou/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)
read data...
x_train shape: (44125, 64, 64, 3)
y_train shape: (44125, 164)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 64, 64, 3)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 64, 64, 16)   448         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 64, 64, 16)   64          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 64, 64, 16)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 64, 64, 16)   2320        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 64, 64, 16)   64          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 64, 64, 16)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 64, 64, 16)   2320        activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 64, 64, 16)   64          conv2d_3[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 64, 64, 16)   0           activation_1[0][0]               
                                                                 batch_normalization_3[0][0]      
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 64, 64, 16)   0           add_1[0][0]                      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 64, 64, 16)   2320        activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 64, 64, 16)   64          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 64, 64, 16)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 64, 64, 16)   2320        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 64, 64, 16)   64          conv2d_5[0][0]                   
__________________________________________________________________________________________________
add_2 (Add)                     (None, 64, 64, 16)   0           activation_3[0][0]               
                                                                 batch_normalization_5[0][0]      
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 64, 64, 16)   0           add_2[0][0]                      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 64, 64, 16)   2320        activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 64, 64, 16)   64          conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 64, 64, 16)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 64, 64, 16)   2320        activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 64, 64, 16)   64          conv2d_7[0][0]                   
__________________________________________________________________________________________________
add_3 (Add)                     (None, 64, 64, 16)   0           activation_5[0][0]               
                                                                 batch_normalization_7[0][0]      
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 64, 64, 16)   0           add_3[0][0]                      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 32)   4640        activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 32, 32, 32)   128         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 32, 32, 32)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 32, 32, 32)   9248        activation_8[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 32, 32, 32)   544         activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 32, 32, 32)   128         conv2d_9[0][0]                   
__________________________________________________________________________________________________
add_4 (Add)                     (None, 32, 32, 32)   0           conv2d_10[0][0]                  
                                                                 batch_normalization_9[0][0]      
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 32, 32, 32)   0           add_4[0][0]                      
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 32, 32, 32)   9248        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 32, 32, 32)   128         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 32, 32, 32)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 32, 32, 32)   9248        activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 32, 32, 32)   128         conv2d_12[0][0]                  
__________________________________________________________________________________________________
add_5 (Add)                     (None, 32, 32, 32)   0           activation_9[0][0]               
                                                                 batch_normalization_11[0][0]     
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 32, 32, 32)   0           add_5[0][0]                      
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 32, 32, 32)   9248        activation_11[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 32, 32, 32)   128         conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 32, 32, 32)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 32, 32, 32)   9248        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 32, 32, 32)   128         conv2d_14[0][0]                  
__________________________________________________________________________________________________
add_6 (Add)                     (None, 32, 32, 32)   0           activation_11[0][0]              
                                                                 batch_normalization_13[0][0]     
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 32, 32, 32)   0           add_6[0][0]                      
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 16, 16, 64)   18496       activation_13[0][0]              
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 16, 16, 64)   256         conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 16, 16, 64)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 16, 16, 64)   36928       activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 16, 16, 64)   2112        activation_13[0][0]              
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 16, 16, 64)   256         conv2d_16[0][0]                  
__________________________________________________________________________________________________
add_7 (Add)                     (None, 16, 16, 64)   0           conv2d_17[0][0]                  
                                                                 batch_normalization_15[0][0]     
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 16, 16, 64)   0           add_7[0][0]                      
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 16, 16, 64)   36928       activation_15[0][0]              
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 16, 16, 64)   256         conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 16, 16, 64)   0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 16, 16, 64)   36928       activation_16[0][0]              
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 16, 16, 64)   256         conv2d_19[0][0]                  
__________________________________________________________________________________________________
add_8 (Add)                     (None, 16, 16, 64)   0           activation_15[0][0]              
                                                                 batch_normalization_17[0][0]     
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 16, 16, 64)   0           add_8[0][0]                      
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 16, 16, 64)   36928       activation_17[0][0]              
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 16, 16, 64)   256         conv2d_20[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 16, 16, 64)   0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 16, 16, 64)   36928       activation_18[0][0]              
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 16, 16, 64)   256         conv2d_21[0][0]                  
__________________________________________________________________________________________________
add_9 (Add)                     (None, 16, 16, 64)   0           activation_17[0][0]              
                                                                 batch_normalization_19[0][0]     
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 16, 16, 64)   0           add_9[0][0]                      
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 4, 4, 64)     0           activation_19[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 4, 4, 64)     0           average_pooling2d_1[0][0]        
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 1024)         0           dropout_1[0][0]                  
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 164)          168100      flatten_1[0][0]                  
==================================================================================================
Total params: 441,892
Trainable params: 440,516
Non-trainable params: 1,376
__________________________________________________________________________________________________
Using real-time data augmentation.
Epoch 1/500
2018-09-19 14:21:29.586991: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-09-19 14:21:29.681418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-19 14:21:29.681914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:04.0
totalMemory: 11.17GiB freeMemory: 11.09GiB
2018-09-19 14:21:29.682063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
Epoch 00001: val_acc improved from -inf to 0.11218, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.001.0.112176.h5
 - 196s - loss: 4.7494 - acc: 0.0655 - val_loss: 4.2990 - val_acc: 0.1122
Epoch 2/500
Epoch 00002: val_acc improved from 0.11218 to 0.15174, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.002.0.151744.h5
 - 191s - loss: 3.8647 - acc: 0.1719 - val_loss: 3.9446 - val_acc: 0.1517
Epoch 3/500
Epoch 00003: val_acc improved from 0.15174 to 0.23312, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.003.0.233123.h5
 - 190s - loss: 3.5059 - acc: 0.2312 - val_loss: 3.4929 - val_acc: 0.2331
Epoch 4/500
Epoch 00004: val_acc improved from 0.23312 to 0.25005, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.004.0.250051.h5
 - 191s - loss: 3.2876 - acc: 0.2698 - val_loss: 3.4131 - val_acc: 0.2501
Epoch 5/500
Epoch 00005: val_acc improved from 0.25005 to 0.27555, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.005.0.275546.h5
 - 190s - loss: 3.1218 - acc: 0.3007 - val_loss: 3.2736 - val_acc: 0.2755
Epoch 6/500
Epoch 00006: val_acc improved from 0.27555 to 0.28350, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.006.0.283500.h5
 - 190s - loss: 2.9882 - acc: 0.3254 - val_loss: 3.2488 - val_acc: 0.2835
Epoch 7/500
Epoch 00007: val_acc improved from 0.28350 to 0.30655, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.007.0.306547.h5
 - 190s - loss: 2.8616 - acc: 0.3495 - val_loss: 3.1423 - val_acc: 0.3065
Epoch 8/500
Epoch 00008: val_acc improved from 0.30655 to 0.32266, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.008.0.322660.h5
 - 190s - loss: 2.7595 - acc: 0.3699 - val_loss: 3.0220 - val_acc: 0.3227
Epoch 9/500
Epoch 00009: val_acc did not improve
 - 190s - loss: 2.6756 - acc: 0.3881 - val_loss: 3.1066 - val_acc: 0.3116
Epoch 10/500
Epoch 00010: val_acc improved from 0.32266 to 0.34367, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.010.0.343667.h5
 - 190s - loss: 2.5964 - acc: 0.4044 - val_loss: 2.9249 - val_acc: 0.3437
Epoch 11/500
Epoch 00011: val_acc did not improve
 - 191s - loss: 2.5280 - acc: 0.4204 - val_loss: 3.0287 - val_acc: 0.3365
Epoch 12/500
Epoch 00012: val_acc did not improve
 - 190s - loss: 2.4605 - acc: 0.4340 - val_loss: 3.1120 - val_acc: 0.3274
Epoch 13/500
Epoch 00013: val_acc improved from 0.34367 to 0.34979, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.013.0.349786.h5
 - 190s - loss: 2.4113 - acc: 0.4467 - val_loss: 3.1050 - val_acc: 0.3498
Epoch 14/500
Epoch 00014: val_acc improved from 0.34979 to 0.37304, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.014.0.373037.h5
 - 190s - loss: 2.3548 - acc: 0.4565 - val_loss: 2.8375 - val_acc: 0.3730
Epoch 15/500
Epoch 00015: val_acc did not improve
 - 190s - loss: 2.3061 - acc: 0.4690 - val_loss: 2.9807 - val_acc: 0.3520
Epoch 16/500
Epoch 00016: val_acc did not improve
 - 190s - loss: 2.2599 - acc: 0.4815 - val_loss: 2.9387 - val_acc: 0.3649
Epoch 17/500
Epoch 00017: val_acc did not improve
 - 190s - loss: 2.2240 - acc: 0.4895 - val_loss: 2.9596 - val_acc: 0.3592
Epoch 18/500
Epoch 00018: val_acc improved from 0.37304 to 0.39874, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.018.0.398735.h5
 - 191s - loss: 2.1824 - acc: 0.4966 - val_loss: 2.7585 - val_acc: 0.3987
Epoch 19/500
Epoch 00019: val_acc did not improve
 - 191s - loss: 2.1474 - acc: 0.5008 - val_loss: 2.9134 - val_acc: 0.3785
Epoch 20/500
Epoch 00020: val_acc improved from 0.39874 to 0.39955, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.020.0.399551.h5
 - 191s - loss: 2.1202 - acc: 0.5101 - val_loss: 2.7866 - val_acc: 0.3996
Epoch 21/500
Epoch 00021: val_acc improved from 0.39955 to 0.40812, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.021.0.408117.h5
 - 191s - loss: 2.0949 - acc: 0.5169 - val_loss: 2.7420 - val_acc: 0.4081
Epoch 22/500
Epoch 00022: val_acc did not improve
 - 191s - loss: 2.0613 - acc: 0.5212 - val_loss: 2.8345 - val_acc: 0.3953
Epoch 23/500
Epoch 00023: val_acc did not improve
 - 191s - loss: 2.0350 - acc: 0.5306 - val_loss: 2.8517 - val_acc: 0.3957
Epoch 24/500
Epoch 00024: val_acc did not improve
 - 191s - loss: 2.0090 - acc: 0.5362 - val_loss: 2.9264 - val_acc: 0.3800
Epoch 25/500
Epoch 00025: val_acc did not improve
 - 191s - loss: 1.9938 - acc: 0.5427 - val_loss: 2.7950 - val_acc: 0.4051
Epoch 26/500
Epoch 00026: val_acc did not improve
 - 191s - loss: 1.9639 - acc: 0.5474 - val_loss: 2.7770 - val_acc: 0.4059
Epoch 27/500
Epoch 00027: val_acc did not improve
 - 191s - loss: 1.9505 - acc: 0.5514 - val_loss: 2.9054 - val_acc: 0.3781
Epoch 28/500
Epoch 00028: val_acc improved from 0.40812 to 0.42239, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.028.0.422394.h5
 - 191s - loss: 1.7650 - acc: 0.5952 - val_loss: 2.7126 - val_acc: 0.4224
Epoch 29/500
Epoch 00029: val_acc improved from 0.42239 to 0.42892, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.029.0.428921.h5
 - 191s - loss: 1.7261 - acc: 0.6026 - val_loss: 2.7155 - val_acc: 0.4289
Epoch 30/500
Epoch 00030: val_acc improved from 0.42892 to 0.43504, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.030.0.435040.h5
 - 191s - loss: 1.6905 - acc: 0.6098 - val_loss: 2.7199 - val_acc: 0.4350
Epoch 31/500
Epoch 00031: val_acc did not improve
 - 191s - loss: 1.6660 - acc: 0.6134 - val_loss: 2.7260 - val_acc: 0.4291
Epoch 32/500
Epoch 00032: val_acc improved from 0.43504 to 0.44646, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.032.0.446461.h5
 - 191s - loss: 1.6489 - acc: 0.6194 - val_loss: 2.6612 - val_acc: 0.4465
Epoch 33/500
Epoch 00033: val_acc improved from 0.44646 to 0.45095, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.033.0.450948.h5
 - 191s - loss: 1.6393 - acc: 0.6171 - val_loss: 2.6304 - val_acc: 0.4509
Epoch 34/500
Epoch 00034: val_acc did not improve
 - 191s - loss: 1.6114 - acc: 0.6261 - val_loss: 2.7301 - val_acc: 0.4367
Epoch 35/500
Epoch 00035: val_acc did not improve
 - 191s - loss: 1.5952 - acc: 0.6278 - val_loss: 2.7015 - val_acc: 0.4381
Epoch 36/500
Epoch 00036: val_acc did not improve
 - 191s - loss: 1.5832 - acc: 0.6319 - val_loss: 3.0587 - val_acc: 0.3934
Epoch 37/500
Epoch 00037: val_acc did not improve
 - 191s - loss: 1.5749 - acc: 0.6346 - val_loss: 2.7110 - val_acc: 0.4442
Epoch 38/500
Epoch 00038: val_acc did not improve
 - 191s - loss: 1.5541 - acc: 0.6369 - val_loss: 2.7389 - val_acc: 0.4354
Epoch 39/500
Epoch 00039: val_acc did not improve
 - 191s - loss: 1.5466 - acc: 0.6396 - val_loss: 2.8584 - val_acc: 0.4169
Epoch 40/500
Epoch 00040: val_acc did not improve
 - 191s - loss: 1.4335 - acc: 0.6652 - val_loss: 2.7211 - val_acc: 0.4450
Epoch 41/500
Epoch 00041: val_acc improved from 0.45095 to 0.46074, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.041.0.460738.h5
 - 191s - loss: 1.4046 - acc: 0.6749 - val_loss: 2.6994 - val_acc: 0.4607
Epoch 42/500
Epoch 00042: val_acc did not improve
 - 191s - loss: 1.3786 - acc: 0.6798 - val_loss: 2.6852 - val_acc: 0.4489
Epoch 43/500
Epoch 00043: val_acc did not improve
 - 191s - loss: 1.3876 - acc: 0.6792 - val_loss: 2.7051 - val_acc: 0.4516
Epoch 44/500
Epoch 00044: val_acc did not improve
 - 190s - loss: 1.3629 - acc: 0.6824 - val_loss: 2.6942 - val_acc: 0.4524
Epoch 45/500
Epoch 00045: val_acc improved from 0.46074 to 0.46257, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.045.0.462574.h5
 - 190s - loss: 1.3034 - acc: 0.6964 - val_loss: 2.6734 - val_acc: 0.4626
Epoch 46/500
Epoch 00046: val_acc did not improve
 - 190s - loss: 1.2911 - acc: 0.7021 - val_loss: 2.6739 - val_acc: 0.4599
Epoch 47/500
Epoch 00047: val_acc did not improve
 - 190s - loss: 1.2747 - acc: 0.7042 - val_loss: 2.6634 - val_acc: 0.4622
Epoch 48/500
Epoch 00048: val_acc improved from 0.46257 to 0.46400, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.048.0.464002.h5
 - 190s - loss: 1.2666 - acc: 0.7050 - val_loss: 2.6741 - val_acc: 0.4640
Epoch 49/500
Epoch 00049: val_acc did not improve
 - 191s - loss: 1.2587 - acc: 0.7087 - val_loss: 2.6865 - val_acc: 0.4614
Epoch 50/500
Epoch 00050: val_acc improved from 0.46400 to 0.46828, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.050.0.468285.h5
 - 193s - loss: 1.2330 - acc: 0.7157 - val_loss: 2.6675 - val_acc: 0.4683
Epoch 51/500
Epoch 00051: val_acc did not improve
 - 193s - loss: 1.2233 - acc: 0.7181 - val_loss: 2.6609 - val_acc: 0.4646
Epoch 52/500
Epoch 00052: val_acc did not improve
 - 194s - loss: 1.2144 - acc: 0.7206 - val_loss: 2.6781 - val_acc: 0.4673
Epoch 53/500
Epoch 00053: val_acc did not improve
 - 194s - loss: 1.2052 - acc: 0.7203 - val_loss: 2.6850 - val_acc: 0.4658
Epoch 54/500
Epoch 00054: val_acc did not improve
 - 194s - loss: 1.2092 - acc: 0.7232 - val_loss: 2.6654 - val_acc: 0.4677
Epoch 55/500
Epoch 00055: val_acc improved from 0.46828 to 0.46890, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.055.0.468897.h5
 - 194s - loss: 1.1840 - acc: 0.7254 - val_loss: 2.6661 - val_acc: 0.4689
Epoch 56/500
Epoch 00056: val_acc improved from 0.46890 to 0.46971, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.056.0.469712.h5
 - 194s - loss: 1.1863 - acc: 0.7255 - val_loss: 2.6643 - val_acc: 0.4697
Epoch 57/500
Epoch 00057: val_acc improved from 0.46971 to 0.47114, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.057.0.471140.h5
 - 194s - loss: 1.1744 - acc: 0.7281 - val_loss: 2.6728 - val_acc: 0.4711
Epoch 58/500
Epoch 00058: val_acc did not improve
 - 194s - loss: 1.1834 - acc: 0.7263 - val_loss: 2.6601 - val_acc: 0.4677
Epoch 59/500
Epoch 00059: val_acc did not improve
 - 194s - loss: 1.1739 - acc: 0.7280 - val_loss: 2.6690 - val_acc: 0.4699
Epoch 60/500
Epoch 00060: val_acc improved from 0.47114 to 0.47338, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.060.0.473384.h5
 - 194s - loss: 1.1664 - acc: 0.7297 - val_loss: 2.6663 - val_acc: 0.4734
Epoch 61/500
Epoch 00061: val_acc did not improve
 - 194s - loss: 1.1697 - acc: 0.7308 - val_loss: 2.6683 - val_acc: 0.4728
Epoch 62/500
Epoch 00062: val_acc did not improve
 - 193s - loss: 1.1649 - acc: 0.7311 - val_loss: 2.6745 - val_acc: 0.4730
Epoch 63/500
Epoch 00063: val_acc did not improve
 - 194s - loss: 1.1649 - acc: 0.7311 - val_loss: 2.6721 - val_acc: 0.4730
Epoch 64/500
Epoch 00064: val_acc did not improve
 - 194s - loss: 1.1699 - acc: 0.7304 - val_loss: 2.6749 - val_acc: 0.4705
Epoch 65/500
Epoch 00065: val_acc improved from 0.47338 to 0.47583, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.065.0.475831.h5
 - 194s - loss: 1.1639 - acc: 0.7330 - val_loss: 2.6718 - val_acc: 0.4758
Epoch 66/500
Epoch 00066: val_acc did not improve
 - 194s - loss: 1.1573 - acc: 0.7348 - val_loss: 2.6699 - val_acc: 0.4736
Epoch 67/500
Epoch 00067: val_acc did not improve
 - 193s - loss: 1.1593 - acc: 0.7340 - val_loss: 2.6740 - val_acc: 0.4732
Epoch 68/500
Epoch 00068: val_acc improved from 0.47583 to 0.47624, saving model to /home/lechongzhou/TC_ZS/Classification_Model/saved_models/cifar10_ResNet_B1_Resnet3n_1_model.068.0.476239.h5
 - 194s - loss: 1.1629 - acc: 0.7333 - val_loss: 2.6733 - val_acc: 0.4762
Epoch 69/500
Epoch 00069: val_acc did not improve
 - 194s - loss: 1.1406 - acc: 0.7385 - val_loss: 2.6723 - val_acc: 0.4752
Epoch 70/500
Epoch 00070: val_acc did not improve
 - 193s - loss: 1.1549 - acc: 0.7329 - val_loss: 2.6733 - val_acc: 0.4740
Epoch 71/500
Epoch 00071: val_acc did not improve
 - 194s - loss: 1.1580 - acc: 0.7319 - val_loss: 2.6733 - val_acc: 0.4736
Epoch 72/500
Epoch 00072: val_acc did not improve
 - 194s - loss: 1.1474 - acc: 0.7362 - val_loss: 2.6725 - val_acc: 0.4730
Epoch 73/500
Epoch 00073: val_acc did not improve
 - 194s - loss: 1.1582 - acc: 0.7327 - val_loss: 2.6734 - val_acc: 0.4738
Epoch 74/500
Epoch 00074: val_acc did not improve
 - 194s - loss: 1.1451 - acc: 0.7367 - val_loss: 2.6748 - val_acc: 0.4724
Epoch 75/500
Epoch 00075: val_acc did not improve
 - 193s - loss: 1.1451 - acc: 0.7362 - val_loss: 2.6739 - val_acc: 0.4736
Epoch 76/500
Epoch 00076: val_acc did not improve
 - 193s - loss: 1.1483 - acc: 0.7355 - val_loss: 2.6740 - val_acc: 0.4722
Epoch 77/500
Epoch 00077: val_acc did not improve
 - 194s - loss: 1.1470 - acc: 0.7371 - val_loss: 2.6740 - val_acc: 0.4722
Epoch 78/500
Epoch 00078: val_acc did not improve
 - 194s - loss: 1.1442 - acc: 0.7357 - val_loss: 2.6739 - val_acc: 0.4724
Epoch 79/500
Epoch 00079: val_acc did not improve
 - 194s - loss: 1.1407 - acc: 0.7383 - val_loss: 2.6757 - val_acc: 0.4722
Epoch 80/500
Epoch 00080: val_acc did not improve
 - 194s - loss: 1.1446 - acc: 0.7379 - val_loss: 2.6768 - val_acc: 0.4726
Epoch 81/500
Epoch 00081: val_acc did not improve
 - 194s - loss: 1.1455 - acc: 0.7353 - val_loss: 2.6747 - val_acc: 0.4724
Epoch 82/500
Epoch 00082: val_acc did not improve
 - 193s - loss: 1.1500 - acc: 0.7352 - val_loss: 2.6771 - val_acc: 0.4713
Epoch 83/500
Epoch 00083: val_acc did not improve
 - 192s - loss: 1.1501 - acc: 0.7345 - val_loss: 2.6743 - val_acc: 0.4720
Epoch 84/500
Epoch 00084: val_acc did not improve
 - 190s - loss: 1.1464 - acc: 0.7343 - val_loss: 2.6760 - val_acc: 0.4720
Epoch 85/500
Epoch 00085: val_acc did not improve
 - 190s - loss: 1.1459 - acc: 0.7361 - val_loss: 2.6753 - val_acc: 0.4715
Epoch 86/500
Epoch 00086: val_acc did not improve
 - 190s - loss: 1.1437 - acc: 0.7365 - val_loss: 2.6746 - val_acc: 0.4726
Epoch 87/500
Epoch 00087: val_acc did not improve
 - 190s - loss: 1.1495 - acc: 0.7339 - val_loss: 2.6753 - val_acc: 0.4724
Epoch 88/500
Epoch 00088: val_acc did not improve
 - 192s - loss: 1.1505 - acc: 0.7350 - val_loss: 2.6772 - val_acc: 0.4715
Epoch 89/500
Epoch 00089: val_acc did not improve
 - 191s - loss: 1.1474 - acc: 0.7367 - val_loss: 2.6766 - val_acc: 0.4711
Epoch 90/500
Epoch 00090: val_acc did not improve
 - 191s - loss: 1.1448 - acc: 0.7359 - val_loss: 2.6763 - val_acc: 0.4718
Epoch 91/500
Epoch 00091: val_acc did not improve
 - 189s - loss: 1.1524 - acc: 0.7352 - val_loss: 2.6755 - val_acc: 0.4728
Epoch 92/500
Epoch 00092: val_acc did not improve
 - 191s - loss: 1.1453 - acc: 0.7363 - val_loss: 2.6752 - val_acc: 0.4715
Epoch 93/500
Epoch 00093: val_acc did not improve
 - 191s - loss: 1.1479 - acc: 0.7350 - val_loss: 2.6767 - val_acc: 0.4720
Epoch 94/500
Epoch 00094: val_acc did not improve
 - 191s - loss: 1.1495 - acc: 0.7354 - val_loss: 2.6743 - val_acc: 0.4720
Epoch 95/500
Epoch 00095: val_acc did not improve
 - 191s - loss: 1.1450 - acc: 0.7353 - val_loss: 2.6769 - val_acc: 0.4715
Epoch 96/500
Epoch 00096: val_acc did not improve
 - 191s - loss: 1.1509 - acc: 0.7338 - val_loss: 2.6755 - val_acc: 0.4728
Epoch 97/500
Epoch 00097: val_acc did not improve
 - 190s - loss: 1.1484 - acc: 0.7356 - val_loss: 2.6765 - val_acc: 0.4722
Epoch 98/500
Epoch 00098: val_acc did not improve
 - 189s - loss: 1.1448 - acc: 0.7345 - val_loss: 2.6758 - val_acc: 0.4722
Epoch 99/500
Epoch 00099: val_acc did not improve
 - 191s - loss: 1.1474 - acc: 0.7354 - val_loss: 2.6762 - val_acc: 0.4713
Epoch 100/500
Epoch 00100: val_acc did not improve
 - 191s - loss: 1.1484 - acc: 0.7349 - val_loss: 2.6783 - val_acc: 0.4711
Epoch 101/500
Epoch 00101: val_acc did not improve
 - 191s - loss: 1.1479 - acc: 0.7328 - val_loss: 2.6758 - val_acc: 0.4718
Epoch 102/500
Epoch 00102: val_acc did not improve
 - 191s - loss: 1.1408 - acc: 0.7371 - val_loss: 2.6778 - val_acc: 0.4707
Epoch 103/500
Epoch 00103: val_acc did not improve
 - 192s - loss: 1.1434 - acc: 0.7367 - val_loss: 2.6779 - val_acc: 0.4720
Epoch 104/500
Epoch 00104: val_acc did not improve
 - 191s - loss: 1.1479 - acc: 0.7368 - val_loss: 2.6762 - val_acc: 0.4713
Epoch 105/500
Epoch 00105: val_acc did not improve
 - 191s - loss: 1.1436 - acc: 0.7351 - val_loss: 2.6783 - val_acc: 0.4713
Epoch 106/500
Epoch 00106: val_acc did not improve
 - 191s - loss: 1.1389 - acc: 0.7378 - val_loss: 2.6765 - val_acc: 0.4724
Epoch 107/500
Epoch 00107: val_acc did not improve
 - 191s - loss: 1.1337 - acc: 0.7417 - val_loss: 2.6769 - val_acc: 0.4718
Epoch 108/500
Epoch 00108: val_acc did not improve
 - 191s - loss: 1.1479 - acc: 0.7352 - val_loss: 2.6765 - val_acc: 0.4724
Epoch 109/500
Epoch 00109: val_acc did not improve
 - 191s - loss: 1.1502 - acc: 0.7362 - val_loss: 2.6774 - val_acc: 0.4718
Epoch 110/500
Epoch 00110: val_acc did not improve
 - 191s - loss: 1.1439 - acc: 0.7362 - val_loss: 2.6755 - val_acc: 0.4738
Epoch 111/500
Epoch 00111: val_acc did not improve
 - 191s - loss: 1.1420 - acc: 0.7390 - val_loss: 2.6756 - val_acc: 0.4720
Epoch 112/500
Epoch 00112: val_acc did not improve
 - 191s - loss: 1.1438 - acc: 0.7378 - val_loss: 2.6794 - val_acc: 0.4722
Epoch 113/500
Epoch 00113: val_acc did not improve
 - 191s - loss: 1.1538 - acc: 0.7334 - val_loss: 2.6776 - val_acc: 0.4728
Epoch 114/500
Epoch 00114: val_acc did not improve
 - 191s - loss: 1.1469 - acc: 0.7358 - val_loss: 2.6766 - val_acc: 0.4722
Epoch 115/500
Epoch 00115: val_acc did not improve
 - 191s - loss: 1.1432 - acc: 0.7387 - val_loss: 2.6759 - val_acc: 0.4728
Epoch 116/500
Epoch 00116: val_acc did not improve
 - 191s - loss: 1.1464 - acc: 0.7347 - val_loss: 2.6770 - val_acc: 0.4722
Epoch 117/500
Epoch 00117: val_acc did not improve
 - 191s - loss: 1.1441 - acc: 0.7368 - val_loss: 2.6794 - val_acc: 0.4732
Epoch 118/500
Epoch 00118: val_acc did not improve
 - 191s - loss: 1.1424 - acc: 0.7375 - val_loss: 2.6778 - val_acc: 0.4724
Epoch 119/500
Epoch 00119: val_acc did not improve
 - 191s - loss: 1.1461 - acc: 0.7354 - val_loss: 2.6776 - val_acc: 0.4724
Epoch 120/500
Epoch 00120: val_acc did not improve
 - 191s - loss: 1.1433 - acc: 0.7357 - val_loss: 2.6781 - val_acc: 0.4724
Epoch 121/500
Epoch 00121: val_acc did not improve
 - 191s - loss: 1.1451 - acc: 0.7377 - val_loss: 2.6781 - val_acc: 0.4713
Epoch 122/500
Epoch 00122: val_acc did not improve
 - 190s - loss: 1.1477 - acc: 0.7340 - val_loss: 2.6779 - val_acc: 0.4740
Epoch 123/500
Epoch 00123: val_acc did not improve
 - 190s - loss: 1.1420 - acc: 0.7351 - val_loss: 2.6784 - val_acc: 0.4736
Epoch 124/500
Epoch 00124: val_acc did not improve
 - 191s - loss: 1.1448 - acc: 0.7357 - val_loss: 2.6774 - val_acc: 0.4728
Epoch 125/500
Epoch 00125: val_acc did not improve
 - 191s - loss: 1.1433 - acc: 0.7355 - val_loss: 2.6791 - val_acc: 0.4724
Epoch 126/500
Epoch 00126: val_acc did not improve
 - 192s - loss: 1.1421 - acc: 0.7362 - val_loss: 2.6787 - val_acc: 0.4728
Epoch 127/500
Epoch 00127: val_acc did not improve
 - 191s - loss: 1.1419 - acc: 0.7387 - val_loss: 2.6773 - val_acc: 0.4726
Epoch 128/500
Epoch 00128: val_acc did not improve
 - 191s - loss: 1.1429 - acc: 0.7362 - val_loss: 2.6789 - val_acc: 0.4703
Epoch 129/500
Epoch 00129: val_acc did not improve
 - 191s - loss: 1.1484 - acc: 0.7362 - val_loss: 2.6764 - val_acc: 0.4724
Epoch 130/500
Epoch 00130: val_acc did not improve
 - 191s - loss: 1.1446 - acc: 0.7338 - val_loss: 2.6776 - val_acc: 0.4724
Epoch 131/500
